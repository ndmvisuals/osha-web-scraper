{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re\n",
    "import pathlib\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Webscraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseWebscraper():\n",
    "    def __init__(self, data_directory):\n",
    "        \n",
    "        self.parent_directory = pathlib.Path(data_directory)          \n",
    "        \n",
    "    # Scraper ------------------------------------------------\n",
    "    ## This section of the class handles the downloading of html or files from websites\n",
    "    def _make_request_soup(self):\n",
    "        \"\"\"\n",
    "        description: uses requests library to download html\n",
    "        input: url defined by scrape \n",
    "        output: html of webpage        \n",
    "        \"\"\"\n",
    "        r = requests.get(self.url)\n",
    "        self.html = r.content        \n",
    "\n",
    "    def _make_selenium_soup(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: uses selenium library to download html\n",
    "        input: url defined by scrape \n",
    "        output: html of webpage        \n",
    "        \"\"\"\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(\n",
    "                ChromeDriverManager().install(),\n",
    "                options=chrome_options\n",
    "                )\n",
    "        driver.get(self.url)\n",
    "        self.html = driver.page_source \n",
    "\n",
    "    def _get_csv(self):\n",
    "        \"\"\"\n",
    "        description: uses pandas library to download csv file from website\n",
    "        input: url defined by scrape\n",
    "        output: pandas dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df_csv = pd.read_csv(self.url)\n",
    "        return(df_csv)  \n",
    "\n",
    "    def _tag_visible(self,element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]', 'table']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def scrape(self,url, scraper_type, name, save_folder):\n",
    "        \"\"\"\n",
    "        description: saves html code from website to specified folder\n",
    "        input:\n",
    "            - url: url of the website\n",
    "            - scraper_type: requests or selenium as a string\n",
    "            - name: name of the file\n",
    "            - save_folder: the name of the folder the file wil save in with respect to data directory already defined\n",
    "        output: saved html file on local disk        \n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        if scraper_type == \"requests\":      \n",
    "            self.soup = self._make_request_soup()\n",
    "        if scraper_type == \"selenium\":\n",
    "            self.soup = self._make_selenium_soup()\n",
    "        self.save_html(name, save_folder)\n",
    "        \n",
    "\n",
    "\n",
    "    def _text_from_html(self):\n",
    "    \n",
    "        texts = self.soup.findAll(text=True)\n",
    "        visible_texts = filter(self._tag_visible, texts)  \n",
    "        return u\" \".join(t.strip() for t in visible_texts)     \n",
    "        \n",
    "\n",
    "   \n",
    "    def _save_file(self, save_folder):\n",
    "        self.file_path = self.parent_directory.joinpath(save_folder)\n",
    "        self.file_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "\n",
    "    def save_html(self,name, save_folder):\n",
    "        self._save_file(save_folder)\n",
    "        full_name = name + \".html\"        \n",
    "        file_path_final = self.file_path.joinpath(full_name)\n",
    "\n",
    "        with open(file_path_final, 'wb') as f:\n",
    "            f.write(self.html)\n",
    "\n",
    "    def _save_txt(self,text,name, save_folder):\n",
    "        self._save_file(save_folder)\n",
    "        full_name = name + \".txt\"        \n",
    "        file_path_final = self.file_path.joinpath(full_name)\n",
    "        \n",
    "\n",
    "        with open(file_path_final, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "    # Parser ----------------------------------------\n",
    "    ## This section of the class handles parsing html files to get data\n",
    "\n",
    "    def _make_parse_soup(self, file_name):\n",
    "        with open(file_name) as fp:\n",
    "            html = fp      \n",
    "\n",
    "            soup = BeautifulSoup(html, 'html.parser') \n",
    "            return(soup)          \n",
    "\n",
    "        \n",
    "\n",
    "    def _get_table(self, table_element, option):\n",
    "        ls_table_tr = table_element.find_all(\"tr\")\n",
    "        rows = []\n",
    "        for tr in ls_table_tr:\n",
    "            row = []\n",
    "            count = 0\n",
    "            for child in tr.children:\n",
    "                if count == 0:\n",
    "                    result = child.text.replace('\\n', '')\n",
    "                    if option == 1:\n",
    "                        if result != \"\":\n",
    "                            row.append(result)\n",
    "                    if option == 2:\n",
    "                        row.append(result)\n",
    "                else: \n",
    "\n",
    "                    try:\n",
    "                        row.append(child.text.replace('\\n', ''))\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            count = count + 1\n",
    "            if len(row) > 0:\n",
    "                rows.append(row)     \n",
    "        \n",
    "        # diff_btwn_col_row = len(rows[0]) - len(rows[1])\n",
    "        # if diff_btwn_col_row == 0:\n",
    "        df = pd.DataFrame(rows[1:], columns = rows[0])\n",
    "        df_table = df\n",
    "        # else:\n",
    "        #     column = rows[0][diff_btwn_col_row:]\n",
    "        #     df = pd.DataFrame(rows[1:], columns = column)\n",
    "        #     df_table = df\n",
    "        return(df_table)\n",
    "\n",
    "    def _find_table_by_title(self,soup,tag, title):\n",
    "        try:\n",
    "            item = soup.find(string = re.compile(title))\n",
    "            ls_parents = item.parents\n",
    "            \n",
    "            for parent in ls_parents:\n",
    "                if parent.find(tag):\n",
    "                    return(parent)\n",
    "                    break\n",
    "        except:        \n",
    "            return(\"no table found\")\n",
    "\n",
    "   \n",
    "\n",
    "    def get_files_in_folder(self,folder_path):\n",
    "        path = pathlib.Path(folder_path)   \n",
    "        files = [e for e in path.iterdir() if e.is_file()]\n",
    "        return(files)\n",
    "            \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat OSHA class\n",
    "- This class inherits functions from the base class\n",
    "- Also added to the class are custom functions to handle the website that general functions don't cover\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OshaScraperParser(BaseWebscraper):\n",
    "    def __init__(self, data_directory):\n",
    "        super().__init__(data_directory)\n",
    "    \n",
    "    # THis part of scraper includes functions for parsing and returning the links from result page to scrape\n",
    "    def parse_links(self, link):\n",
    "        \n",
    "        soup = self._make_parse_soup(link)\n",
    "        ls_tables = soup.find_all(\"div\", {\"class\":\"table-responsive\"})\n",
    "        result_table = ls_tables[1]\n",
    "        ls_links = result_table.find_all(\"a\")\n",
    "        ls_urls = []\n",
    "        for link in ls_links:    \n",
    "            url = link[\"href\"]\n",
    "            if \"establishment\" in url:\n",
    "                url_start = \"http://www.osha.gov/pls/imis/\"\n",
    "                ls_urls.append(url_start + url)\n",
    "        return(ls_urls)\n",
    "    def download_link_html(self, ls_links):\n",
    "        for url in tqdm(ls_links):\n",
    "            url_split = url.split(\"=\")\n",
    "            case_number = url_split[1].strip()\n",
    "            self.scrape(url, \"requests\", case_number, \"dollar_tree_cases\")\n",
    "\n",
    "    # This part of scraper includes functions specific to osha for parsing and returning information for each case\n",
    "    def _parse_inspection_information(self,line):\n",
    "    \n",
    "        inspection_information_dict = {}\n",
    "        ls_split_line = line.split(\":\")\n",
    "        key = ls_split_line[0].strip()\n",
    "        \n",
    "        value = ls_split_line[1].strip()\n",
    "\n",
    "        if key == \"Related Activity\":\n",
    "            return(\"Related Activity\")\n",
    "        \n",
    "        elif value != \"\":              \n",
    "            return(ls_split_line)\n",
    "        else: # need to go to sibling for value of the key\n",
    "            if key == \"Scope\":\n",
    "                value = line.parent.find_all(\"td\")[0].text\n",
    "            else:\n",
    "                try:\n",
    "                    value = line.parent.next_sibling.text\n",
    "                except:\n",
    "                    value = \"\"\n",
    "        \n",
    "            return([key, value])\n",
    "    def _get_inspection_information(self,inspection_table):\n",
    "\n",
    "        hr_2 = inspection_table.find_all(\"hr\")[1]\n",
    "        ls_name_tr = hr_2.parent.find_all(\"tr\")\n",
    "        company_name = ls_name_tr[0].text\n",
    "        address = ls_name_tr[1].get_text(separator=\" \").strip().split(\"\\n\")[0]    \n",
    "\n",
    "        ls_keys = inspection_table.find_all(string = re.compile(\":\"))    \n",
    "        ls_inspection_information = []\n",
    "        inspection_information_dict ={}\n",
    "\n",
    "        inspection_information_dict[\"address\"] = address\n",
    "\n",
    "        for i in ls_keys:   \n",
    "            \n",
    "            result = self._parse_inspection_information(i)\n",
    "            if result == \"Related Activity\":\n",
    "                continue\n",
    "            else:\n",
    "                inspection_information_dict[result[0]] = result[1]\n",
    "        ls_inspection_information.append(inspection_information_dict)      \n",
    "        df_inspection_information = pd.DataFrame.from_dict(ls_inspection_information)\n",
    "        return(df_inspection_information)\n",
    "\n",
    "    def _get_related_activity_from_inspection_information(self,inspection_table):\n",
    "        ls_keys = inspection_table.find_all(string = re.compile(\":\"))\n",
    "        for i in ls_keys:           \n",
    "            result = self._parse_inspection_information(i)\n",
    "            if result == \"Related Activity\":\n",
    "                return(i.parent.parent.parent)\n",
    "\n",
    "    def _get_violation_table(self,table_element):\n",
    "        rows = []\n",
    "        ls_table_tr = table_element.find_all(\"tr\")\n",
    "\n",
    "        for tr in ls_table_tr:\n",
    "            row = []\n",
    "            count = 0\n",
    "            for child in tr.children:              \n",
    "                result = child.text.replace('\\n', '').strip()\n",
    "                row.append(result)\n",
    "            rows.append(row)\n",
    "        # print(rows)\n",
    "\n",
    "        rows_cleaned = []\n",
    "        for row in rows:\n",
    "            row_cleaned =[]\n",
    "            counter = 0\n",
    "            for i in row:\n",
    "                counter = counter +1\n",
    "                if counter %2 == 0:\n",
    "                    row_cleaned.append(i)\n",
    "            rows_cleaned.append(row_cleaned)\n",
    "\n",
    "        df = pd.DataFrame(rows_cleaned[1:], columns = rows_cleaned[0])\n",
    "        df_table = df\n",
    "        return(df_table)\n",
    "\n",
    "    def parse_osha_datatables(self, file_path):\n",
    "\n",
    "        soup = self._make_parse_soup(file_path)\n",
    "\n",
    "        # isolate the section of html for each table ---------------\n",
    "\n",
    "        ## Case Status and Nr\n",
    "        try:\n",
    "            case_status = soup.find(string = re.compile(\"Case Status\")).split(\":\")\n",
    "        except:\n",
    "            case_status = \"info_not_on_page\"\n",
    "\n",
    "        ## Inspection information\n",
    "        inspection_information = self._find_table_by_title(soup,\"table\", \"Inspection Information\")\n",
    "        df_inspection_information = self._get_inspection_information(inspection_information)\n",
    "        df_inspection_information[case_status[0]] = case_status[1]        \n",
    "        df_inspection_information_clean = df_inspection_information.clean_names()\n",
    "        nr = df_inspection_information_clean[\"nr\"][0] # isolate unique id to put in other dataframes\n",
    "\n",
    "        ## Related Activity \n",
    "        try:\n",
    "            related_activity =  self._get_related_activity_from_inspection_information(inspection_information)\n",
    "            df_related_activity =self._get_table(related_activity, 1)\n",
    "            df_related_activity_clean = (df_related_activity\n",
    "            .clean_names()\n",
    "            .add_prefix(\"related_activity_\")\n",
    "            .drop(columns=\"related_activity_related_activity_\")\n",
    "            )\n",
    "            df_related_activity_clean[\"nr\"] = nr # add unique id\n",
    "        except:\n",
    "            df_related_activity_clean = \"no table\"\n",
    "\n",
    "        try:\n",
    "            ## Violation Summary\n",
    "            violation_summary = self._find_table_by_title(soup, \"table\", \"Violation Summary\")\n",
    "            df_violation_summary = self._get_table(violation_summary, 2)\n",
    "            df_violation_summary_clean = df_violation_summary.iloc[:, [1, 3, 5, 7, 9, 11,13]]\n",
    "            df_violation_summary_clean =  df_violation_summary_clean.rename(columns = {df_violation_summary.columns[1]: \"violation_type\"})\n",
    "            df_violation_summary_clean = df_violation_summary_clean.clean_names()\n",
    "            df_violation_summary_clean[\"nr\"] = nr # add unique id\n",
    "        except:\n",
    "            df_violation_summary_clean = \"no table\"\n",
    "\n",
    "        try:\n",
    "            ## Violation Items\n",
    "            violation_items = self._find_table_by_title(soup,\"table\", \"Violation Items\")\n",
    "            df_violation_items = self._get_violation_table(violation_items) \n",
    "            df_violation_items_clean = (df_violation_items\n",
    "            .rename(columns = {\"\":\"notes\"})\n",
    "            .clean_names()\n",
    "            )\n",
    "            df_violation_items_clean[\"nr\"] = nr\n",
    "        except:\n",
    "            df_violation_items_clean = \"no_table\"                \n",
    "\n",
    "      \n",
    "        return(df_inspection_information_clean,df_related_activity_clean,df_violation_summary_clean,df_violation_items_clean )      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an instant of the class and preform scraping steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create instance of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osha_scraper = OshaScraperParser( \"../data/\" ) # create instance of the class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download html of main page that has the links to each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osha_scraper.scrape(\"https://www.osha.gov/pls/imis/establishment.search?establishment=Dollar%20Tree&state=all&officetype=all&office=all&sitezip=100000&startmonth=08&startday=03&startyear=2012&endmonth=08&endday=03&endyear=2022&p_case=all&p_violations_exist=both&p_start=&p_finish=0&p_sort=12&p_desc=DESC&p_direction=Next&p_show=700\", \"requests\",\"dollar_tree_links\", \"html/links/\" ) # download html of page with results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse main page and create list of urls to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_dollar_tree_urls = osha_scraper.parse_links(r\"C:\\\\Users\\\\nicho\\\\Documents\\\\GitHub\\\\presidential_speeches\\\\data\\html\\\\links\\\\dollar_tree_links.html\") # parse result links from the html of page save above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download html page of each url link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osha_scraper.download_link_html(ls_dollar_tree_urls) # download all html files from the results links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all files and create 4 dataframes from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:29<00:00, 22.47it/s]\n"
     ]
    }
   ],
   "source": [
    "ls_dollar_tree_html = osha_scraper.get_files_in_folder(r\"C://Users//nicho//Documents//GitHub//presidential_speeches//data//dollar_tree_cases\") # return all html paths\n",
    "\n",
    "\n",
    "# Scrape each html file and add to table if table exists add ------------------------------\n",
    "counter = 0\n",
    "for file in tqdm(ls_dollar_tree_html): \n",
    "    # print(file)\n",
    "    # print(\"-----\")   \n",
    "    counter = counter + 1\n",
    "    df_inspection_information, df_related_activity, df_violation_summary, df_violation_items = osha_scraper.parse_osha_datatables(file)\n",
    "    if counter == 1:\n",
    "        df_final_inspection_information = df_inspection_information\n",
    "        df_final_related_activity = df_related_activity\n",
    "        df_final_violation_summary = df_violation_summary\n",
    "        df_final_violation_items = df_violation_items\n",
    "    else:\n",
    "        \n",
    "        if isinstance(df_inspection_information, pd.DataFrame) == True: \n",
    "            df_final_inspection_information = pd.concat([df_final_inspection_information, df_inspection_information], ignore_index = True)\n",
    "        if isinstance(df_related_activity, pd.DataFrame) == True: \n",
    "            df_final_related_activity = pd.concat([df_final_related_activity, df_related_activity], ignore_index = True)\n",
    "        if isinstance(df_violation_summary, pd.DataFrame) == True:                       \n",
    "            df_final_violation_summary = pd.concat([df_final_violation_summary, df_violation_summary], ignore_index = True)\n",
    "            \n",
    "        if isinstance(df_violation_items, pd.DataFrame) == True:\n",
    "            df_final_violation_items = pd.concat([df_final_violation_items,df_violation_items], ignore_index = True)      \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_inspection_information.to_csv(\"inspection_information.csv\")\n",
    "df_final_related_activity.to_csv(\"related_activity.csv\")\n",
    "df_final_violation_summary.to_csv(\"violation_summary.csv\")\n",
    "df_final_violation_items.to_csv(\"violation_items.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('presidential_speeches-doVqbMRr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56b2ec469682ff19063fcc6367a0d714d9b5bb9349ab1fcaab36b4e50b68f0bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
